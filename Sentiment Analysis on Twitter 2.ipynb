{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1907c7f3-1586-4bb4-829d-8ccce8ea5169",
   "metadata": {},
   "source": [
    "Continuation of first notebook to improve accuracy of the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "60e95a29-3317-4c9e-8916-25173bc6fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd55e402-6d09-4b06-ad43-e90a778b7c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Tweets.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c44b79d8-d31b-461f-9682-0c173bf158cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df = df.set_axis(['target','id','date','flag','user','text'], axis='columns')\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e95333f-b336-48b6-8fc2-4e2196af566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove mentions\n",
    "def remove_mentions(text):\n",
    "    return re.sub(r'@\\w+', '', text)\n",
    "\n",
    "df['text'] = df['text'].apply(remove_mentions)\n",
    "\n",
    "def clean_text_v2(text):\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"\\w+@\\w+\\.com\", \"\", text)     # Remove emails\n",
    "     # Normalize repeated punctuation (! and ?)\n",
    "    text = re.sub(r\"!{2,}\", \"!\", text)  # Replace multiple exclamation marks with one\n",
    "    text = re.sub(r\"\\?{2,}\", \"?\",text)  # Replace multiple question marks with one\n",
    "    text = re.sub(r\"[.,;:\\\"'`]\", \"\", text)     # Remove punctuation  but keep ! and ?\n",
    "    text = re.sub(r\"[@\\$%^&*\\(\\)\\\\/\\+-_=\\[\\]\\{\\}<>]\", \"\", text)  # Remove special chars\n",
    "\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    return text.strip()\n",
    "df['text'] = df['text'].apply(clean_text_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da21f316-5c28-427a-bc11-2632ef201cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#80/20 training split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['text'], df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "798dffc3-5e8e-42ed-8169-1deb9733484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_1 = TfidfVectorizer(max_features=20000, ngram_range = (1,2)).fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "67aa0394-cfa6-43e8-a79a-c6a4ca971026",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vectorized = tfidf_1.transform(x_train)\n",
    "x_test_vectorized = tfidf_1.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8fb7bfcd-e9e8-46c3-8a2d-989914778294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[159717  39998]\n",
      " [ 34980 165305]]\n",
      "\n",
      "Accuracy: \n",
      " 81.26 %\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='saga',max_iter=1000 )\n",
    "model.fit(x_train_vectorized, y_train)\n",
    "predictions = model.predict(x_test_vectorized)\n",
    "\n",
    "print('Confusion Matrix: \\n', confusion_matrix(y_test, predictions))\n",
    "\n",
    "print('\\nAccuracy: \\n', round(accuracy_score(y_test, predictions) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7a38f24-bfc4-4158-a682-19c54b8825df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_length'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fa2fd42a-ae8a-4d58-833a-427e70faa189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "df['sentiment_score'] = df['text'].apply(lambda x: sia.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be06f50d-3bcf-4ec3-93f3-3d2b76c830f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['exclamation_count'] = df['text'].apply(lambda x: x.count('!'))\n",
    "df['question_count'] = df['text'].apply(lambda x: x.count('?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2bdc1852-07a9-41c0-bc12-444e0f81480d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of structured_features_train: (1199999, 5)\n",
      "Shape of structured_features_test: (400000, 5)\n"
     ]
    }
   ],
   "source": [
    "# Extract structured features for the training data\n",
    "structured_features_train = df.loc[x_train.index, ['text_length', 'word_count', 'sentiment_score', 'exclamation_count', 'question_count']].values\n",
    "\n",
    "# Extract structured features for the test data\n",
    "structured_features_test = df.loc[x_test.index, ['text_length', 'word_count', 'sentiment_score', 'exclamation_count', 'question_count']].values\n",
    "\n",
    "print(\"Shape of structured_features_train:\", structured_features_train.shape)\n",
    "print(\"Shape of structured_features_test:\", structured_features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3881dcc4-97f3-4245-9c94-8cc36dd6d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale structured features\n",
    "scaler = StandardScaler()\n",
    "structured_features_train2 = scaler.fit_transform(structured_features_train)\n",
    "structured_features_test2 = scaler.transform(structured_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ec70c2f6-7066-49d6-b8ae-9e688cf6fc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.46 %\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Combine TF-IDF features with structured features\n",
    "structured_features = df[['text_length', 'word_count', 'sentiment_score', 'exclamation_count', 'question_count']].values\n",
    "x_train_combined = hstack([x_train_vectorized, structured_features_train2])\n",
    "x_test_combined = hstack([x_test_vectorized, structured_features_test2])\n",
    "\n",
    "# Train Logistic Regression\n",
    "model3 = LogisticRegression(solver='saga', max_iter=1000)\n",
    "model3.fit(x_train_combined, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "prediction3 = model3.predict(x_test_combined)\n",
    "accuracy = accuracy_score(y_test, prediction3)\n",
    "print(\"Accuracy:\", round(accuracy * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be27f80-6d88-4288-b717-355ed704c1de",
   "metadata": {},
   "source": [
    "Too slow runtime for only a marginal increase in accuracy. Better to go with the first model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
